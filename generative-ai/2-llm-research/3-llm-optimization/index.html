<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, viewport-fit=cover"
    />

    <style>
      :root {
        --accent-color: #05a081;
        --accent-color-light: #82d0c0;
        --accent-overlay-color: #fff;
        --body-bg: #fff;
        --body-color: #000;
        --heading-color: #000;
        --table-bg-even: #f3f3f3;
        --table-border-bottom: #dddddd;
      }
      
    </style>

    <meta name="theme-color" content="#05a081" />

    
      <link rel="icon" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.5bc282474aa9c014.jpg" />
      <link rel="apple-touch-icon" sizes="48x48" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.5bc282474aa9c014.jpg" />
      <link rel="apple-touch-icon" sizes="72x72" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.fe16e0bb11047769.jpg" />
      <link rel="apple-touch-icon" sizes="96x96" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.605594d8be6a111f.jpg" />
      <link rel="apple-touch-icon" sizes="144x144" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.3d1b2eeabf8eca47.jpg" />
      <link rel="apple-touch-icon" sizes="192x192" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.b6439cbc27d70267.jpg" />
      <link rel="apple-touch-icon" sizes="256x256" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.e69fb5473e9ea744.jpg" />
      <link rel="apple-touch-icon" sizes="384x384" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.d03b11f47410640c.jpg" />
      <link rel="apple-touch-icon" sizes="512x512" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.d847062c67ed9f5e.jpg" />
      
    

    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-JD979V1BWS"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-JD979V1BWS');
      </script>
    

    <meta property="og:type" content="website">

    <meta name="twitter:card" content="summary">

    

    

    
      
        <meta name="description" content="" />
        <meta name="twitter:description" content="">
      
    

    
      <meta name="twitter:title" content="LLM Optimization">
    

    
      <link rel="prerender" href="&#x2F;blog&#x2F;" />
    
      <link rel="prerender" href="&#x2F;books&#x2F;" />
    
      <link rel="prerender" href="&#x2F;interviews&#x2F;" />
    
      <link rel="prerender" href="&#x2F;generative-ai&#x2F;" />
    

    <link rel="prefetch" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.db45dd197188a7fb.jpg" />

    <title>
      
    
    
    
    Gal's 
     
    
        Generative AI
    
  Book - LLM Optimization
    

    </title>

    
    
<link rel="stylesheet" href="https://gel.github.io/book.css">
      


     





  </head>
  <body>
    

    <main>
    
        <div class="menu">
            
            
            <nav role="navigation">
                <ul>
                    <li>
                        <a href="/">
                            ‚Üê Back to Blog
                        </a>
                    </li>
                    
                        
                        
                        
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;1-intro&#x2F;">
                                    <strong>1.</strong>
                                    Generative AI Foundations
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;1-intro&#x2F;1-llm-survey&#x2F;">
                                                    <strong>1.1.</strong>
                                                    LLM Survey
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;">
                                    <strong>2.</strong>
                                    LLM Research
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;1-llm-pretraining-finetuning&#x2F;">
                                                    <strong>2.1.</strong>
                                                    LLM Pretraining &amp; Fine-tuning
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;2-llm-agents&#x2F;">
                                                    <strong>2.2.</strong>
                                                    LLM Agents
                                                </a>
                                            </li>
                                        
                                            <li class="active">
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;3-llm-optimization&#x2F;">
                                                    <strong>2.3.</strong>
                                                    LLM Optimization
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;4-llm-prompting&#x2F;">
                                                    <strong>2.4.</strong>
                                                    LLM Prompting
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;5-llm-benchmarks&#x2F;">
                                                    <strong>2.5.</strong>
                                                    LLM Benchmarks &amp; Evaludations
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;6-llm-multimodal&#x2F;">
                                                    <strong>2.6.</strong>
                                                    LLM Multi-Modal
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;7-llm-models&#x2F;">
                                                    <strong>2.7.</strong>
                                                    LLM Models
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;">
                                    <strong>3.</strong>
                                    LLM Implementation
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;1-datasets&#x2F;">
                                                    <strong>3.1.</strong>
                                                    Datasets
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;2-llms&#x2F;">
                                                    <strong>3.2.</strong>
                                                    Large-Language-Models
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                                           
                    
                </ul>
            </nav>
            
            
        </div>

        <div class="page">
            <div class="page__header">
                <div class="menu-icon">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
                
                <span class="search-icon">üîé</span>
                
            </div>

            <div class="page__content">
                
                <div class="search-container">
                    <input id="search" type="search" placeholder="Search..">
                    <div class="search-results">
                        <div class="search-results__header"></div>
                        <ul class="search-results__items"></ul>
                    </div>
                </div>
                
                <div class="book-content">
                    
    <h1>LLM Optimization</h1>
    <h3 id="llm-in-a-flash-efficient-llm-inference-with-limited-memory">[LLM-in-a-Flash] Efficient LLM Inference with Limited Memory<a class="zola-anchor" href="#llm-in-a-flash-efficient-llm-inference-with-limited-memory" aria-label="Anchor link for: llm-in-a-flash-efficient-llm-inference-with-limited-memory">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2312.11514">https://arxiv.org/abs/2312.11514</a> <em>12 Dec 2023 <strong>Apple</strong></em></p>
<p>First, &quot;windowing'&quot; strategically reduces data transfer by reusing previously activated neurons, and second, &quot;row-column bundling&quot;, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. </p>
<h3 id="rope-roformer-enhanced-transformer-with-rotary-position-embedding">[RoPE] RoFormer: Enhanced Transformer with Rotary Position Embedding<a class="zola-anchor" href="#rope-roformer-enhanced-transformer-with-rotary-position-embedding" aria-label="Anchor link for: rope-roformer-enhanced-transformer-with-rotary-position-embedding">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2104.09864">https://arxiv.org/abs/2104.09864</a> <em>20 Apr 2021 <strong>Zhuiyi Technology Co.</strong></em></p>
<p>We investigated the existing approaches to the relative position encoding and found that they are mostly built based on the idea of the decomposition of adding position encoding to the context representations. We introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. The key idea is to encode relative position by multiplying the context representations with a rotation matrix with a clear theoretical interpretation.</p>
<h3 id="lora-low-rank-adaptation-of-llm">[LORA] LOw-RAnk Adaptation of LLM<a class="zola-anchor" href="#lora-low-rank-adaptation-of-llm" aria-label="Anchor link for: lora-low-rank-adaptation-of-llm">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a> <em>17 Jun 2021 <strong>OpenAI</strong></em></p>
<p>Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning,</p>
<p>which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many</p>
<p>parameters as in the original model.</p>
<p>Many sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques often introduce inference latency by extending model depth or reducing the model‚Äôs usable sequence length. More importantly, these methods often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.</p>
<h3 id="speculative-fast-inference-from-transformers-via-speculative-decoding">[Speculative] Fast Inference from Transformers via Speculative Decoding<a class="zola-anchor" href="#speculative-fast-inference-from-transformers-via-speculative-decoding" aria-label="Anchor link for: speculative-fast-inference-from-transformers-via-speculative-decoding">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2211.17192">https://arxiv.org/abs/2211.17192</a> <em>30 Nov 2022 <strong>Google</strong></em></p>
<p>The key observation above, that some inference steps are ‚Äúharder‚Äù and some are ‚Äúeasier‚Äù, is also a key motivator for our work. We additionally observe that inference from large models is often not bottlenecked on arithmetic operations, but rather on memory bandwidth and communication, so additional computation resources might be available.</p>
<p>Therefore we suggest increasing concurrency as a complementary approach to using an adaptive amount of computation. Specifically, we are able to accelerate inference without changing the model architectures, without changing the training-procedures or needing to re-train the models, and without changing the model output distribution. This is accomplished via speculative execution.</p>
<p><img src="/generative-ai/2-llm-research/llm_3_speculative.png" alt="Speculative Decoding" /></p>
<h3 id="gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints">[GQA] Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints<a class="zola-anchor" href="#gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints" aria-label="Anchor link for: gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2305.13245">https://arxiv.org/abs/2305.13245</a> <em>22 May 2023 <strong>Google</strong></em></p>
<p><img src="/generative-ai/2-llm-research/llm_3_gqa.png" alt="GQA" /></p>
<h3 id="multi-heads-sharing-fast-transformer-decoding-one-write-head-is-all-you-need">[Multi-Heads Sharing]Fast Transformer Decoding: One Write-Head is All You Need<a class="zola-anchor" href="#multi-heads-sharing-fast-transformer-decoding-one-write-head-is-all-you-need" aria-label="Anchor link for: multi-heads-sharing-fast-transformer-decoding-one-write-head-is-all-you-need">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a> <em>6 Nov 2019  <strong>Google</strong></em></p>
<p>Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such parallelization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large &quot;keys'' and &quot;values&quot; tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention &quot;heads&quot;, greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.</p>
<p>We introduce multi-query Attention as a variation of multi-head attention as described in [Vaswani et al., 2017]. Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs. Multi-query attention is identical except that the different heads share a single set of keys and values.</p>
<h3 id="moe-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer">[MoE] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts Layer<a class="zola-anchor" href="#moe-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer" aria-label="Anchor link for: moe-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a> <em>23 Jan 2017 <strong>Google</strong></em></p>
<p>The capacity of a neural network to absorb information is limited by its number of</p>
<p>parameters. Conditional computation, where parts of the network are active on a</p>
<p>per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice,</p>
<p>However, there are significant algorithmic and performance challenges. In this</p>
<p>work, we address these challenges and finally realize the promise of conditional</p>
<p>computation, achieving greater than 1000x improvements in model capacity with</p>
<p>only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to</p>
<p>thousands of feed-forward sub-networks. A trainable gating network determines</p>
<p>a sparse combination of these experts to use for each example. </p>
<p>We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.</p>
<h3 id="moe-mixture-of-experts-meets-instruction-tuning-a-winning-combination-for-llm">[MoE] Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for LLM<a class="zola-anchor" href="#moe-mixture-of-experts-meets-instruction-tuning-a-winning-combination-for-llm" aria-label="Anchor link for: moe-mixture-of-experts-meets-instruction-tuning-a-winning-combination-for-llm">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2305.14705">https://arxiv.org/abs/2305.14705</a> <em>24 May 2023 <strong>Google</strong></em></p>
<p>Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. </p>


                </div>
            </div>

            <div class="prev-link">
                
    
        
        
        <a class="previous" href="https://gel.github.io/generative-ai/2-llm-research/"><</a>
    

            </div>

            <div class="next-link">
                
    
        <a class="next" href="https://gel.github.io/generative-ai/2-llm-research/4-llm-prompting/">></a>
    

            </div>
        </div>

        
            
            <script type="text/javascript" src="https://gel.github.io/elasticlunr.min.js"></script>
            <script type="text/javascript" src="https://gel.github.io/search_index.en.js"></script>
            
            <script type="text/javascript" src="https://gel.github.io/book.js"></script>
            <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

    </main>
    <footer class="footer-page">
    
      
    
    </footer>
  </body>
</html>
