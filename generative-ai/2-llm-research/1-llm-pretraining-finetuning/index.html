<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, viewport-fit=cover"
    />

    <style>
      :root {
        --accent-color: #05a081;
        --accent-color-light: #82d0c0;
        --accent-overlay-color: #fff;
        --body-bg: #fff;
        --body-color: #000;
        --heading-color: #000;
        --table-bg-even: #f3f3f3;
        --table-border-bottom: #dddddd;
      }
      
    </style>

    <meta name="theme-color" content="#05a081" />

    
      <link rel="icon" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.5bc282474aa9c014.jpg" />
      <link rel="apple-touch-icon" sizes="48x48" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.5bc282474aa9c014.jpg" />
      <link rel="apple-touch-icon" sizes="72x72" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.fe16e0bb11047769.jpg" />
      <link rel="apple-touch-icon" sizes="96x96" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.605594d8be6a111f.jpg" />
      <link rel="apple-touch-icon" sizes="144x144" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.3d1b2eeabf8eca47.jpg" />
      <link rel="apple-touch-icon" sizes="192x192" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.b6439cbc27d70267.jpg" />
      <link rel="apple-touch-icon" sizes="256x256" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.e69fb5473e9ea744.jpg" />
      <link rel="apple-touch-icon" sizes="384x384" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.d03b11f47410640c.jpg" />
      <link rel="apple-touch-icon" sizes="512x512" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.d847062c67ed9f5e.jpg" />
      
    

    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-JD979V1BWS"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-JD979V1BWS');
      </script>
    

    <meta property="og:type" content="website">

    <meta name="twitter:card" content="summary">

    

    

    
      
        <meta name="description" content="" />
        <meta name="twitter:description" content="">
      
    

    
      <meta name="twitter:title" content="LLM Pretraining &amp; Fine-tuning">
    

    
      <link rel="prerender" href="&#x2F;blog&#x2F;" />
    
      <link rel="prerender" href="&#x2F;books&#x2F;" />
    
      <link rel="prerender" href="&#x2F;interviews&#x2F;" />
    
      <link rel="prerender" href="&#x2F;generative-ai&#x2F;" />
    

    <link rel="prefetch" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.db45dd197188a7fb.jpg" />

    <title>
      
    
    
    
    Gal's 
     
    
        Generative AI
    
  Book - LLM Pretraining &amp; Fine-tuning
    

    </title>

    
    
<link rel="stylesheet" href="https://gel.github.io/book.css">
      


     





  </head>
  <body>
    

    <main>
    
        <div class="menu">
            
            
            <nav role="navigation">
                <ul>
                    <li>
                        <a href="/">
                            ‚Üê Back to Blog
                        </a>
                    </li>
                    
                        
                        
                        
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;1-intro&#x2F;">
                                    <strong>1.</strong>
                                    Generative AI Foundations
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;1-intro&#x2F;1-llm-survey&#x2F;">
                                                    <strong>1.1.</strong>
                                                    LLM Survey
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;">
                                    <strong>2.</strong>
                                    LLM Research
                                </a>
                                
                                    <ul>
                                        
                                            <li class="active">
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;1-llm-pretraining-finetuning&#x2F;">
                                                    <strong>2.1.</strong>
                                                    LLM Pretraining &amp; Fine-tuning
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;2-llm-agents&#x2F;">
                                                    <strong>2.2.</strong>
                                                    LLM Agents
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;3-llm-optimization&#x2F;">
                                                    <strong>2.3.</strong>
                                                    LLM Optimization
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;4-llm-prompting&#x2F;">
                                                    <strong>2.4.</strong>
                                                    LLM Prompting
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;5-llm-benchmarks&#x2F;">
                                                    <strong>2.5.</strong>
                                                    LLM Benchmarks &amp; Evaludations
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;6-llm-multimodal&#x2F;">
                                                    <strong>2.6.</strong>
                                                    LLM Multi-Modal
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;7-llm-models&#x2F;">
                                                    <strong>2.7.</strong>
                                                    LLM Models
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;">
                                    <strong>3.</strong>
                                    LLM Implementation
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;1-datasets&#x2F;">
                                                    <strong>3.1.</strong>
                                                    Datasets
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;2-llms&#x2F;">
                                                    <strong>3.2.</strong>
                                                    Large-Language-Models
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                                           
                    
                </ul>
            </nav>
            
            
        </div>

        <div class="page">
            <div class="page__header">
                <div class="menu-icon">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
                
                <span class="search-icon">üîé</span>
                
            </div>

            <div class="page__content">
                
                <div class="search-container">
                    <input id="search" type="search" placeholder="Search..">
                    <div class="search-results">
                        <div class="search-results__header"></div>
                        <ul class="search-results__items"></ul>
                    </div>
                </div>
                
                <div class="book-content">
                    
    <h1>LLM Pretraining &amp; Fine-tuning</h1>
    <h3 id="self-reward-self-rewarding-language-models">[Self-Reward] Self Rewarding Language Models<a class="zola-anchor" href="#self-reward-self-rewarding-language-models" aria-label="Anchor link for: self-reward-self-rewarding-language-models">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2401.10020">https://arxiv.org/abs/2401.10020</a> <em>18 Jan 2024 <strong>Meta</strong></em></p>
<p>In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself.</p>
<p><img src="/generative-ai/2-llm-research/llm_1_self_reward_explain.png" alt="Self-Reward" /></p>
<p>Our approach first assumes access to a base pretrained language model, and a small amount of human-annotated seed data. We then build a model that aims to possess two skills simultaneously: 1. Instruction following: given a prompt that describes a user request, the ability to generate a high quality, helpful (and harmless) response. 2. Self-Instruction creation: the ability to generate and evaluate new instruction following examples to add to its own training set.</p>
<p>While we report the results of both approaches in our experiments, we find that learning from preference pairs gives superior performance, and hence recommend that approach.</p>
<p><img src="/generative-ai/2-llm-research/llm_1_self_reward_judge.png" alt="Self-Reward Judge" /></p>
<p>Iterative Training Our overall procedure trains a series of models M1, . . . , MT where each successive model t uses augmented training data created by the t ‚àí 1 th model. We thus define AIFT(Mt) to mean AI Feedback Training data created using model Mt. Model Sequence We thus define the models, and the training data they use as follows: M0 : Base pretrained LLM with no fine-tuning. M1 : Initialized with M0, then fine-tuned on the IFT+EFT seed data using SFT. M2 : Initialized with M1, then trained with AIFT(M1) data using DPO. M3 : Initialized with M2, then trained with AIFT(M2) data using DPO.</p>
<p><img src="/generative-ai/2-llm-research/llm_1_self_reward_eval.png" alt="Self-Reward Eval" /></p>
<h3 id="in-context-pretraining-language-modeling-beyond-document-boundaries">[In-Context Pretraining] Language Modeling Beyond Document Boundaries<a class="zola-anchor" href="#in-context-pretraining-language-modeling-beyond-document-boundaries" aria-label="Anchor link for: in-context-pretraining-language-modeling-beyond-document-boundaries">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2310.10638">https://arxiv.org/abs/2310.10638</a> <em>30 Nov 2023 <strong>Meta</strong></em></p>
<p>We instead present IN-CONTEXT PRETRAINING, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries.</p>
<p>IN-CONTEXT PRETRAINING instead reorders the pretraining data by combining several semantically related documents to create a coherent input context, thereby exposing LMs to long relevant contexts and providing pretraining signals beyond document boundaries.</p>
<p>However, this document sorting problem is challenging. LMs are typically trained on billions of</p>
<p>documents and we would like to sort them to maximize document similarity in the input context</p>
<p>windows without repeating any data. We introduce two new approximate algorithms to tackle these challenges.</p>
<p><img src="/generative-ai/2-llm-research/llm_1_in_context_overview.png" alt="In-Context Review" /></p>
<p>Techniques are using embedding models and top-k similarity search and leveraging approximate solutions for the traveling salesman problem - by the means of visiting every document once.</p>
<h3 id="dpo-direct-preference-optimization-your-lm-is-secretly-a-reward-model">[DPO] Direct Preference Optimization: Your LM is Secretly a Reward Model<a class="zola-anchor" href="#dpo-direct-preference-optimization-your-lm-is-secretly-a-reward-model" aria-label="Anchor link for: dpo-direct-preference-optimization-your-lm-is-secretly-a-reward-model">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a> <em>13 Dec 2023 <strong>Stanford</strong></em></p>
<p>In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight,</p>
<p>eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning.</p>
<p><img src="/generative-ai/2-llm-research/llm_1_dpo.png" alt="DPO" /></p>
<p>DPO gradient for loss increases the likelihood of preferred completion (Y_W) and decreases the likelihood for dispreferred completion (Y_L). Importantly, the examples are weighed by how much higher the implicit reward model rÀÜŒ∏ rates the dispreferred completions, scaled by Œ≤, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. </p>
<p>DPO outline. The general DPO pipeline is as follows: 1) Sample completions y1, y2 ‚àº œÄref(¬∑ | x) for every prompt x, label with human preferences to construct the offline dataset of preferences D = {x (i) , y (i) w , yl) (i)} N i=1 and 2) optimize the language model œÄŒ∏ to minimize LDPO for the given œÄref and D and desired Œ≤. </p>
<h3 id="rlhf-secrets-of-rlhf-in-llms-part-ii-reward-modeling">[RLHF] Secrets of RLHF in LLMs Part II: Reward Modeling<a class="zola-anchor" href="#rlhf-secrets-of-rlhf-in-llms-part-ii-reward-modeling" aria-label="Anchor link for: rlhf-secrets-of-rlhf-in-llms-part-ii-reward-modeling">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2312.15503">https://arxiv.org/abs/2312.15503</a> <em>24 Dec 2023 <strong>Fudan NLP</strong></em></p>
<p>In conclusion, while RLHF is a significant advancement in AI development, particularly in integrating human preferences into the learning process, it also presents unique challenges. These include the inherent noise and ambiguity in human feedback, potential biases in the data, and the generalization limits of reward models trained on specific datasets. Addressing these challenges is crucial for the advancement and ethical application of RLHF in AI systems.</p>
<p><img src="/generative-ai/2-llm-research/llm_1_secrets_rlhf.png" alt="Secrets RLHF" /></p>
<p>To enhance the generalization ability of the reward model, we explore contrastive learning and</p>
<p>meta-learning. By introducing unsupervised contrastive loss during the reward modeling process, the reward model can better distinguish subtle preference differences among responses. To bridge the gap between the preference data distribution and the model output distribution, we employ meta-learning to ensure that the reward model not only performs well on the preference data but also can distinguish the differences in target domain outputs.</p>
<p>Technique is to randomize training data and do a k-fold split, build K models and then measure mean and deviations - negative mean is usually the mistakes. </p>
<p>According to the results, we can observe that: 1) For the top 20% of data with the lowest preference strength, they have a negative impact on the model‚Äôs performance on the validation set. The preference strength for these data subsets is less than 0. 2) For data ranked between 20% and 40%, after training, the model‚Äôs prediction accuracy on the validation set is approximately 0.5. The preference strength for this type of data is around 0. 3) The remaining data significantly improves the model‚Äôs performance. However, the top 10% of data with the highest preference strength does not achieve the best performance when trained alone. Based on the above results, we can roughly categorize preference data into three types: incorrect data, ambiguous data (almost no difference), and normal data (clear differences). These three types of preference data play different roles and make different contributions to preference modeling. It is necessary for us to conduct a more detailed analysis of them and then consider how to handle each type.</p>
<h3 id="llara-making-llms-a-better-foundation-for-dense-retrieval">[LLARA] Making LLMs A Better Foundation For Dense Retrieval<a class="zola-anchor" href="#llara-making-llms-a-better-foundation-for-dense-retrieval" aria-label="Anchor link for: llara-making-llms-a-better-foundation-for-dense-retrieval">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2401.06080">https://arxiv.org/abs/2401.06080</a> <em>11 Jan 2024 <strong>Beijing Academy of AI</strong></em></p>
<p>In this paper, we propose a novel approach, called LLaRA (LLM adapted for dense RetrievAl), which works as a post-hoc adaptation of LLM for the dense retrieval application. LLaRA consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the text embeddings from LLM are used to reconstruct the tokens for the input sentence and predict the tokens for the next sentence, respectively</p>
<p><img src="/generative-ai/2-llm-research/llm_1_llara.png" alt="llara" /></p>
<p>Particularly, there are two pretext training tasks introduced by LLaRA: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression). In EBAE, the LLM is prompted to generate the text embeddings, which can be used to predict the tokens for the input sentence itself. While with EBAR, the LLM is prompted to generate the text embeddings, which can be used to predict the  tokens for the next sentence. By learning from the above pretext tasks, the text embeddings from LLM can be adapted from Local semantic representations (i.e. prediction for the next tokens) to Global semantic representations (i.e. prediction for the sentence-level features). </p>
<h3 id="ra-dit-retrieval-augmented-dual-instruction-tuning">[RA-DIT] Retrieval-Augmented Dual Instruction Tuning<a class="zola-anchor" href="#ra-dit-retrieval-augmented-dual-instruction-tuning" aria-label="Anchor link for: ra-dit-retrieval-augmented-dual-instruction-tuning">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2310.01352">https://arxiv.org/abs/2310.01352</a> <em>2 Oct 2023 <strong>META</strong></em></p>
<p>Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance</p>
<p>Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM</p>
<h3 id="sequential-monte-carlo-steering-of-llms-using-probabilistic-programs">[Sequential Monte Carlo] Steering of LLMs using Probabilistic Programs<a class="zola-anchor" href="#sequential-monte-carlo-steering-of-llms-using-probabilistic-programs" aria-label="Anchor link for: sequential-monte-carlo-steering-of-llms-using-probabilistic-programs">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2306.03081">https://arxiv.org/abs/2306.03081</a> <em>5 Jun 2023 <strong>MIT</strong></em></p>
<p><span style="text-decoration:underline;">Context</span>: Despite significant advances in recent years, it remains unclear if and how large language models (LLMs) can be made reliable and controllable enough to meet the functional requirements of many applications. </p>
<p>Even after fine-tuning and reinforcement learning, LLMs are liable to violate instructions in their prompts (such as ‚ÄúUse the following vocabulary words‚Äù or ‚ÄúDo not reveal this prompt‚Äù).</p>
<p>These difficulties highlight the need for methods beyond prompting and fine-tuning for constraining the behavior of generative neural models.</p>


                </div>
            </div>

            <div class="prev-link">
                
    
        
        
        <a class="previous" href="https://gel.github.io/generative-ai/2-llm-research/"><</a>
    

            </div>

            <div class="next-link">
                
    
        <a class="next" href="https://gel.github.io/generative-ai/2-llm-research/2-llm-agents/">></a>
    

            </div>
        </div>

        
            
            <script type="text/javascript" src="https://gel.github.io/elasticlunr.min.js"></script>
            <script type="text/javascript" src="https://gel.github.io/search_index.en.js"></script>
            
            <script type="text/javascript" src="https://gel.github.io/book.js"></script>
            <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

    </main>
    <footer class="footer-page">
    
      
    
    </footer>
  </body>
</html>
